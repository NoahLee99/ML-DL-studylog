{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoNHC3mFW36ARMKf+YPtnk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoahLee99/ML-DL-studylog/blob/main/%EB%A6%AC%EB%B7%B0_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%9D%B4%EC%A7%84_%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트의 토큰화"
      ],
      "metadata": {
        "id": "zT3xuExS1ZLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 자연어 처리를 위해서는 먼저 텍스트를 잘게 나누는 전처리를 해야 한다.\n",
        "\n",
        "* 이를 단어별, 문장별, 형태소별로 나눌 수 있는데,\n",
        "<br> 이렇게 작게 나누어진 단위를 **토큰(token)**이라 한다.\n",
        "\n",
        "* 자연어 처리에 사용되는 텍스트 데이터는 **말뭉치(corpus)**라고 한다.\n",
        "\n",
        "* 케라스 text 모듈의 **text_to_word_sequence()** 함수는 문장을 단어 단위로 나누게 해준다.\n",
        "\n",
        "* **Bag-of-Words는 텍스트를 쪼개 가장 많이 쓰인(최고 빈도수) 단어를 찾아\n",
        "<br> 중요도를 파악하는 전처리 방법이다.\n",
        "\n",
        "* **Tokenizer()** 함수의 **fit_on_texts()** 메서드는 텍스트를 토큰화 시키고, 각 토큰에 고유한 인덱스를 부여한다.\n",
        "<br> --> 일종의 단어 사전을 생성하여 텍스트를 벡터값으로 변환 가능\n",
        "\n",
        "* **word_counts** 함수는 단어의 빈도수를 계산해 준다.\n",
        "\n",
        "* **documnet_count()** 함수는 총 문장의 수를 계산해 준다.\n",
        "\n",
        "* **word_docs()** 함수는 각 단어들이 포함된 문장의 수를 계산해 준다.\n",
        "\n",
        "* **word_index()** 함수는 각 단어에 매겨진 인덱스 값을 알려준다."
      ],
      "metadata": {
        "id": "kch8DYIq1cOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 전처리\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "text = \"해보지 않으면 해낼 수 없다\"\n",
        "\n",
        "result = text_to_word_sequence(text) # 텍스트 토큰화\n",
        "print(\"\\n원문:\\n\", text)\n",
        "print(\"\\n토큰화:\\n\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsZlWSaA1a4Z",
        "outputId": "5d3e66e0-15fa-486d-f01e-9ff192716041"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "원문:\n",
            " 해보지 않으면 해낼 수 없다\n",
            "\n",
            "토큰화:\n",
            " ['해보지', '않으면', '해낼', '수', '없다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag-of-Words(단어 빈도수 세기) 전처리\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "docs = [\"먼저 텍스트의 각 단어를 나누어 토큰화합니다.\",\n",
        "        \"텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.\",\n",
        "        \"토큰화한 결과는 딥러닝에서 사용할 수 있습니다.\",\n",
        "        ]\n",
        "\n",
        "token = Tokenizer()\n",
        "\n",
        "token.fit_on_texts(docs) # 토큰화 함수에 문장 적용\n",
        "\n",
        "print(\"\\n단어 카운트:\", token.word_counts) # 단어의 빈도수 계산 결과 출력\n",
        "\n",
        "# OrderedDict 클래스에 담겨 출력됨 (일종의 단어 사전이 생성됨)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3koRSLl2Zqt",
        "outputId": "46f9cecf-839f-42c9-c75c-1dcdae46c41c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "단어 카운트: OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 수 세기\n",
        "print(\"\\n문장 카운트: \", token.document_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M6vsJcG9dHr",
        "outputId": "54fb2841-1c0c-446c-efb4-ee16ae423f78"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "문장 카운트:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 단어가 포함된 문장 수 세기\n",
        "print(\"\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n\", token.word_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkhsHYqu9tdx",
        "outputId": "96f0b6e2-f797-41e1-814a-dbbb128e1ba8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "각 단어가 몇 개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'토큰화합니다': 1, '나누어': 1, '각': 1, '먼저': 1, '텍스트의': 2, '단어를': 1, '딥러닝에서': 2, '토큰화해야': 1, '인식됩니다': 1, '단어로': 1, '결과는': 1, '사용할': 1, '있습니다': 1, '수': 1, '토큰화한': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 단어의 인덱스 값 출력\n",
        "print(\"\\n각 단어에 매겨진 인덱스 값:\\n\", token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMhAcWcu-FpH",
        "outputId": "dc996a40-8091-4bae-a340-ba10fb3b757a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단어의 원-핫 인코딩"
      ],
      "metadata": {
        "id": "7xRrNAaD-4s7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 이전에 배운 원-핫 인코딩과 같은 개념이며, 이것을 단어 배열에 적용한 것이다.\n",
        "\n",
        "* 각 단어가 배열(문장) 내에서 해당하는 위치를 1로 바꾸어 벡터화 한다.\n",
        "\n",
        "* 주의할 점은 벡터 공간 내에서 맨 앞자리는 인덱스 0이 위치한다.\n",
        "\n",
        "* 예를 들어 \"나는 이준호라는 사람이다\"라는 문장을 원-핫 인코딩하면,\n",
        "<br> [\"나는\", \"이준호\", \"사람이다\"]로 토큰화된 다음\n",
        "<br> 1을 더해(맨 앞 인덱스 0) 4차원(3+1) 벡터로 표현된다."
      ],
      "metadata": {
        "id": "b43kNIUU-9HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 원-핫 인코딩 과정\n",
        "\n",
        "# 토큰화 및 각 단어 인덱스 값 출력\n",
        "text = \"오랫동안 꿈꾸는 이는 그 꿈을 닮아간다\"\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts([text]) # 리스트로 감싸지 않으면 \"글자\" 단위로 토큰화함\n",
        "\n",
        "print(token.word_index)\n",
        "\n",
        "# 토큰의 인덱스로 채워진 배열 생성\n",
        "x = token.texts_to_sequences([text])\n",
        "\n",
        "print(x)\n",
        "\n",
        "# 원-핫 인코딩 적용\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "word_size = len(token.word_index) + 1 # 벡터 배열 맨 앞에 0이 추가되므로 1을 더해 줌 (주의)\n",
        "x = to_categorical(x, num_classes=word_size) # num_classes로 벡터 차원(요소 수) 지정\n",
        "\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hwcAoDQAXOx",
        "outputId": "f08d4219-e2df-4401-ad05-f281a4ee5812"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n",
            "[[1, 2, 3, 4, 5, 6]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단어 임베딩"
      ],
      "metadata": {
        "id": "PGBNNhbQCeos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 원-핫 인코딩을 그대로 사용하면 벡터의 길이가 너무 길어지는 단점이 있다.\n",
        "\n",
        "* 이러한 공간적 낭비를 해결하기 위해 등장한 것이 단어 임베딩이다.\n",
        "\n",
        "* **단어 임베딩(word embedding)**이란 각 단어 간의 유사도를 계산하여, 주어진 배열을 정해진 길이로 압축시키는 것이다.\n",
        "\n",
        "* 즉 맥락과 단어 간의 의미를 고려하여 계산 효율을 높이는 방법인 것이다.\n",
        "\n",
        "* 케라스의 **Embedding()** 함수를 통해 단어 임베딩을 적용 가능하다. (입력, 출력, 단어 수)\n",
        "\n",
        "* 시퀀스 길이(토큰 수)를 지정하려면 input_length 매개변수를 설정하면 됨"
      ],
      "metadata": {
        "id": "QzI5Ps8hCgrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트를 읽고 긍정, 부정 예측하기"
      ],
      "metadata": {
        "id": "-81FhVvqKpC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 딥러닝 모델을 학습시키기 위해서는 학습 데이터의 길이가 동일해야 한다.\n",
        "\n",
        "* 따라서 토큰의 수를 똑같이 맞추어 주기 위해 **패딩(padding)** 과정을 거친다.\n",
        "\n",
        "* CNN에서 커널을 적용할 때 사용하는 그 패딩 과정이라 보면 된다.\n",
        "\n",
        "* 시퀀스 길이(토큰 수)를 맞추기 위해 길이가 짧은 부분은 숫자 0을 채워 넣고, 길이가 긴 부분은 잘라서 같은 길이로 **절단(truncating)**하여 맞춘다.\n",
        "\n",
        "* 케라스의 **pad_sequences()** 함수를 사용하여 패딩 가능하다.\n",
        "\n",
        "* 케라스의 **truncating** 매개변수를 사용하여 절단 가능하다."
      ],
      "metadata": {
        "id": "BVrgDQdLMzIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "단어 임베딩을 포함하여 지금까지 배운 내용을 모두 적용해\n",
        "텍스트를 읽고 감정을 예측하는 딥러닝 모델을 만들어 보자.\n",
        "\n",
        "영화를 보고 남긴 리뷰를 딥러닝 모델로 학습하여\n",
        "각 리뷰가 긍정적인지 부정적인지 예측하는 것이다.\n",
        "\n",
        "먼저 짧은 리뷰 열 개를 불러와 각각 긍정이면 1, 부정이라는 0이라는 클래스로 지정한다.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "B2so4L4QKr9d",
        "outputId": "a6198454-49f4-4b69-85e6-e3f1995cf8c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n단어 임베딩을 포함하여 지금까지 배운 내용을 모두 적용해\\n텍스트를 읽고 감정을 예측하는 딥러닝 모델을 만들어 보자.\\n\\n영화를 보고 남긴 리뷰를 딥러닝 모델로 학습하여\\n각 리뷰가 긍정적인지 부정적인지 예측하는 것이다.\\n\\n먼저 짧은 리뷰 열 개를 불러와 각각 긍정이면 1, 부정이라는 0이라는 클래스로 지정한다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,Embedding\n",
        "from numpy import array\n",
        "\n",
        "# 텍스트 리뷰 자료 지정\n",
        "docs = [\"너무 재밌네요\", \"최고예요\", \"참 잘 만든 영화예요\",\n",
        "        \"추천하고 싶은 영화입니다.\", \"한 번 더 보고 싶네요\",\n",
        "        \"글쎄요\", \"별로예요\", \"생각보다 지루하네요\",\n",
        "        \"연기가 어색해요\", \"재미없어요\"]\n",
        "\n",
        "# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스 지정\n",
        "classes = array([1,1,1,1,1,0,0,0,0,0]) # 딥러닝 모델의 타깃(정답) 데이터로 사용\n",
        "\n",
        "# 토큰화 진행\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "\n",
        "print(token.word_index) # 토큰화 결과"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl63_OJlK-iU",
        "outputId": "8a6ba432-24b1-4d29-8b08-dac98d0ecaf0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한': 11, '번': 12, '더': 13, '보고': 14, '싶네요': 15, '글쎄요': 16, '별로예요': 17, '생각보다': 18, '지루하네요': 19, '연기가': 20, '어색해요': 21, '재미없어요': 22}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰에 지정된 인덱스로 새로운 배열 생성\n",
        "x = token.texts_to_sequences(docs)\n",
        "\n",
        "print(\"\\n리뷰 텍스트, 토큰화 결과:\\n\", x)\n",
        "\n",
        "# 출력 결과, 말뭉치의 토큰 수가 각각 다르다.\n",
        "# 딥러닝에 모델에 입력하려면 학습 데이터의 길이가 동일해야 함 --> 패딩"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBkqkiGfMMLw",
        "outputId": "1d2576ab-1484-4f5d-8b70-246120f773c4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "리뷰 텍스트, 토큰화 결과:\n",
            " [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13, 14, 15], [16], [17], [18, 19], [20, 21], [22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 진행\n",
        "padded_x = pad_sequences(x, 4) # 시퀀스 길이를 모두 4로 맞추기\n",
        "# 딥러닝 모델에 입력할 훈련 데이터로 사용\n",
        "\n",
        "print(\"\\n패딩 결과:\\n\", padded_x)\n",
        "\n",
        "# padding 매개변수에 pre를 지정하면 앞에, post를 지정하면 뒤에 숫자0을 채워 넣는다!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL0q20dPMYhs",
        "outputId": "0f033929-f5bd-4f35-e4e2-c55a6061d950"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "패딩 결과:\n",
            " [[ 0  0  1  2]\n",
            " [ 0  0  0  3]\n",
            " [ 4  5  6  7]\n",
            " [ 0  8  9 10]\n",
            " [12 13 14 15]\n",
            " [ 0  0  0 16]\n",
            " [ 0  0  0 17]\n",
            " [ 0  0 18 19]\n",
            " [ 0  0 20 21]\n",
            " [ 0  0  0 22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 임베딩 적용 및 모델 생성\n",
        "word_size = len(token.word_index) + 1 # 입력 지정\n",
        "\n",
        "# 딥러닝 모델 생성\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(word_size, 8, input_length=4)) # 출력, 단어 수 지정\n",
        "model.add(Flatten()) # Flatten 층 추가\n",
        "# 임베딩하면 3차원(입력(배치 사이즈), 시퀀스 길이, 단어 수))이기 때문에 2차원(입력, 시퀀스 길이*단어 수)으로 변환\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "              metrics=['accuracy']) # 이진 분류 문제(긍정 or 부정)\n",
        "\n",
        "# 모델 학습 실행\n",
        "model.fit(padded_x, classes, epochs=20)\n",
        "\n",
        "# 정확도 측정\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_x, classes)[1]))\n",
        "\n",
        "# 첫 번째 층의 파라미터 값은 word_size(22+1) * 8(벡터 크기)) = 168"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQt2FFtbOK-n",
        "outputId": "c4c9a3da-e9bf-455d-9a27-1d249c74e01d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6911\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6000 - loss: 0.6885\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7000 - loss: 0.6860\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7000 - loss: 0.6834\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8000 - loss: 0.6809\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8000 - loss: 0.6784\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9000 - loss: 0.6759\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8000 - loss: 0.6734\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8000 - loss: 0.6709\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8000 - loss: 0.6684\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8000 - loss: 0.6659\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8000 - loss: 0.6634\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8000 - loss: 0.6609\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8000 - loss: 0.6584\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8000 - loss: 0.6559\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8000 - loss: 0.6534\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8000 - loss: 0.6509\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9000 - loss: 0.6484\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9000 - loss: 0.6459\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9000 - loss: 0.6434\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.9000 - loss: 0.6408\n",
            "\n",
            " Accuracy: 0.9000\n"
          ]
        }
      ]
    }
  ]
}