{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFvFzJyPNlXiOXq6FvAeVK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoahLee99/ML-DL-studylog/blob/main/Chapter%2005-3%20-%20%ED%8A%B8%EB%A6%AC%EC%9D%98%20%EC%95%99%EC%83%81%EB%B8%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "앙상블 학습은 정형 데이터에서 가장 뛰어는 성능을 내는 머신러닝 알고리즘 중 하나이다.\n",
        "대표적인 앙상블 학습은 다음과 같다.\n",
        "# 사이킷런\n",
        "  랜덤 포레스트: 부트스트랩 샘플 사용, 대표적 앙상블 학습 알고리즘\n",
        "  엑스트라 트리: 결정 트리의 노드를 랜덤하게 분할함\n",
        "  그레이디언트 부스팅: 이전 트리의 손실을 보완하는 식으로 얕은 결정 트리를 연속하여 추가함\n",
        "  히스토그램 기반 그레이디언트 부스팅: 훈련 데이터를 256개 정수 구간으로 나누어 빠르고 높은 성능을 냄\n",
        "# 그외 라이브러리\n",
        "  XGBoost\n",
        "  LightGBM\n",
        "\n",
        "키워드: [앙상블 학습, 랜덤 포레스트, 엑스트라 트리, 그레이디언트 부스팅, 히스토그램 기반 그레이디언트 부스팅]\n",
        "\n",
        "핵심 패키지 및 함수: [RandomForestClassifier, ExtraTreesClassifier. GradientBoostingClassifier. HistGradientBoostingClassifier]\n",
        "                          n_estimators 매개변수는 앙상블을 구성할 트리의 개수를 지정한다.(기본값은 100)\n",
        "                          criterion 매개변수는 불순도를 지정하며 기본값은 지니 불순도를 의미하는 'gini'이고 'entropy'를 선택하여 엔트로피 불순도를 사용 가능하다.\n",
        "                          max_depth는 트리가 성장할 최대 깊이를 지정한다. 기본값은 None으로 지정하면 리프 노드를 순수하거나 min_samples_split보다 샘플 개수가 적을 때까지 성장한다.\n",
        "                          min_samples_split은 노드를 나누기 위한 최소 샘플의 개수이다.(기본값은 2)\n",
        "                          max_features 매개변수는 최적의 분할을 위해 탐색할 특성의 개수를 지정한다. 기본값은 auto로 특성 개수의 제곱근이다.\n",
        "                          bootstrap 매개변수는 부트스트랩 샘플을 사용할지 지정한다.(기본값은 True)\n",
        "                          oob_score는 OOB 샘플을 사용하여 훈련한 모델을 평가할지 지정한다.(기본값은 False)\n",
        "                          n_jobs 매개변수는 병령 실행에 사용할 CPU 코어 수를 지정한다. 기본값은 1로 하나의 코어를 사용한다. -1로 지정하면 시스템의 모든 코어를 사용한다.\n",
        "                          loss 매개변수는 손실 함수를 지정한다. 기본값은 로지스틱 손실 함수를 의미하는 'deviance'이다.\n",
        "                          learning_rate 매개변수는 트리가 앙상블에 기여하는 정도를 조절한다.(기본값은 0.1)\n",
        "                          (히스토그램 기반 그레이디언트 부스팅에서는 학습률 또는 감쇠율이라 함)\n",
        "                          subsample 매개변수는 사용할 훈련 세트의 샘플 비율을 지정한다.(기본값은 1.0)\n",
        "                          max_iter는 부스팅 단계를 수행하는 트리의 개수이다.(기본값은 100)\n",
        "                          max_bins는 입력 데이터를 나눌 구간의 개수이다. 기본값은 255이며 이보다 크게 지정할 수 없다. 여기에 1개의 구간이 누락된 값을 위해 추가된다.\n"
      ],
      "metadata": {
        "id": "KdYOjDphvGAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "JeocFUm5aoSC",
        "outputId": "17a0bf9c-fe98-4bb1-fb47-1ab7e6a1db62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n지금까지 나는 k-최근접 이웃, 선형 회귀, 릿지, 라쏘, 다항 회귀, 로지스틱 회귀를 배웠고\\n확률적 경사 하강법 알고리즘을 사용한 분류기와 결정 트리 모델까지 섭렵했다.\\n최근에는 테스트 세트의 사용 없이 모델의 성능을 평가하는 교차 검증과 하이퍼파라미터 튜닝까지 배웠다.\\n\\n이번 장에서는 \"랜덤 포레스트(Random Forest)\"라는 알고리즘을 배울 예쩡이다.\\n이에 대해 배우기 전에 잠시 내가 다루었던 데이터를 되돌아보자.\\n4장까지는 생선의 길이, 높이, 무게 등을 데이터로 사용했다.\\n이 데이터는 CSV 파일에 가지런히 정리되어 있었다.\\n또한 이전 장에서 사용한 와인 데이터도 CSV파일이었다.\\n\\n이러한 형태의 데이터를 \"정형 데이터(structured data)\"라고 부른다.\\n쉽게 말해 어떤 구조로 되어 있다는 뜻이다.\\n이런 데이터는 CSV나 데이터베이스, 혹은 엑셀에 저장하기 쉽다.\\n프로그래머가 다루는 대부분의 데이터가 정형 데이터이다.\\n\\n이와 반대되는 데이터는 \"비정형 데이터(unstructured data)\"라고 부른다.\\n비정형 데이터는 데이터베이스나 엑셀로 표현하기 어려운 것들이다.\\n우리 주위의 책과 같은 텍스트 데이터, 디카로 찍은 사진, 핸드폰으로 듣는 디지털 음악 등이 그 예시들이다.\\n\\n지금까지 배운 머신러닝 알고리즘은 정형 데이터에 잘 맞는다.\\n그중 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘이 \"앙상블 학습(ensemble learning)\"이다.\\n이 알고리즘은 대부분 결정 트리를 기반으로 만들어져 있다.\\n이번 장에서 배울 알고리즘들이 앙상블 학습에 속한다.\\n\\n그렇다면 비정형 데이터는 어떤 알고리즘을 사용해야 할까?\\n바로 7장에서 배울 신경망 알고리즘이다.\\n비정형 데이터는 규칙성을 찾기 어려워 전통적인 머신러닝 방법으로는 모델을 만들기 까다롭다.\\n하지만 신경망 알고리즘의 놀라운 발전 덕에 사진을 인식하고 텍스트를 이해하는 모델을 만들 수 있다.\\n\\n이제 사이킷런에서 제공하는 정형 데이터의 끝판왕인 앙상블 학습 알고리즘을 알아보자.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "지금까지 나는 k-최근접 이웃, 선형 회귀, 릿지, 라쏘, 다항 회귀, 로지스틱 회귀를 배웠고\n",
        "확률적 경사 하강법 알고리즘을 사용한 분류기와 결정 트리 모델까지 섭렵했다.\n",
        "최근에는 테스트 세트의 사용 없이 모델의 성능을 평가하는 교차 검증과 하이퍼파라미터 튜닝까지 배웠다.\n",
        "\n",
        "이번 장에서는 \"랜덤 포레스트(Random Forest)\"라는 알고리즘을 배울 예쩡이다.\n",
        "이에 대해 배우기 전에 잠시 내가 다루었던 데이터를 되돌아보자.\n",
        "4장까지는 생선의 길이, 높이, 무게 등을 데이터로 사용했다.\n",
        "이 데이터는 CSV 파일에 가지런히 정리되어 있었다.\n",
        "또한 이전 장에서 사용한 와인 데이터도 CSV파일이었다.\n",
        "\n",
        "이러한 형태의 데이터를 \"정형 데이터(structured data)\"라고 부른다.\n",
        "쉽게 말해 어떤 구조로 되어 있다는 뜻이다.\n",
        "이런 데이터는 CSV나 데이터베이스, 혹은 엑셀에 저장하기 쉽다.\n",
        "프로그래머가 다루는 대부분의 데이터가 정형 데이터이다.\n",
        "\n",
        "이와 반대되는 데이터는 \"비정형 데이터(unstructured data)\"라고 부른다.\n",
        "비정형 데이터는 데이터베이스나 엑셀로 표현하기 어려운 것들이다.\n",
        "우리 주위의 책과 같은 텍스트 데이터, 디카로 찍은 사진, 핸드폰으로 듣는 디지털 음악 등이 그 예시들이다.\n",
        "\n",
        "지금까지 배운 머신러닝 알고리즘은 정형 데이터에 잘 맞는다.\n",
        "그중 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘이 \"앙상블 학습(ensemble learning)\"이다.\n",
        "이 알고리즘은 대부분 결정 트리를 기반으로 만들어져 있다.\n",
        "이번 장에서 배울 알고리즘들이 앙상블 학습에 속한다.\n",
        "\n",
        "그렇다면 비정형 데이터는 어떤 알고리즘을 사용해야 할까?\n",
        "바로 7장에서 배울 신경망 알고리즘이다.\n",
        "비정형 데이터는 규칙성을 찾기 어려워 전통적인 머신러닝 방법으로는 모델을 만들기 까다롭다.\n",
        "하지만 신경망 알고리즘의 놀라운 발전 덕에 사진을 인식하고 텍스트를 이해하는 모델을 만들 수 있다.\n",
        "\n",
        "이제 사이킷런에서 제공하는 정형 데이터의 끝판왕인 앙상블 학습 알고리즘을 알아보자.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "랜덤 포레스트는 앙상블 학습의 대표 주자 중 하나로 안정적인 성능 덕에 널리 사용되고 있다.\n",
        "이름 자체로 유추 가능하듯이, 랜덤 포레스트는 결정 트리를 랜덤하게 만들어 결정 트리(나무)의 \"숲\"을 만든다.\n",
        "그리고 각 결정 트리의 예측을 사용해 최종 예측을 만든다.\n",
        "그럼 랜덤 포레스트가 어떻게 숲을 구성하는지 알아보자.\n",
        "\n",
        "먼저 랜덤 포레스트는 각 트리를 훈련하기 위한 데이터를 랜덤하게 만드는데,\n",
        "이 데이터를 만드는 방법이 독특하다.\n",
        "우리가 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다.\n",
        "이때 한 샘플이 중복되어 추출될 수도 있다.\n",
        "\n",
        "예를 들어, 1000개의 샘플이 들어있는 가방에서 100개의 샘플을 뽑는다면\n",
        "먼저 1개를 뽑고, 뽑았던 1개를 다시 가방에 넣는다.\n",
        "이런 식으로 계속해서 100개를 가방에서 뽑으면 중복된 샘플을 뽑을 수 있다.\n",
        "이렇게 만들어진 샘플을 \"부트스트랩 샘플(bootstrap sample)\"이라고 부른다.\n",
        "기본적으로 부트스트랩 샘플은 훈련 세트의 크기와 같게 만든다.\n",
        "1000개의 샘플이 들어있는 가방에서 중복하여 1000개의 샘플을 뽑는다.\n",
        "\n",
        "(부트스트랩이란?\n",
        "데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식을 의미한다.)\n",
        "\n",
        "또한 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음\n",
        "이중에서 최선의 분할을 찾는다.\n",
        "분류 모델인 RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 선택한다.\n",
        "즉 4개의 특성이 있다면 노드마다 2개를 랜덤하게 선택하여 사용한다.\n",
        "다만 회귀 모델인 RandomForestRegressor는 전체 특성을 사용한다.\n",
        "\n",
        "사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 이런 방식으로 훈련한다.\n",
        "그다음 분류일 때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼는다.\n",
        "회귀일 때는 단순히 각 트리의 예측을 평균한다.\n",
        "\n",
        "랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과댖거합되는 것을 막아주고,\n",
        "검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다.\n",
        "종종 기본 매개변수 설정만으로도 아주 좋은 결과를 낸다.\n",
        "\n",
        "그럼 이제 사이킷런의 RandomForestCalssifier 클래스를 화이트 와인을 분류하는 문제에 적용해 보자.\n",
        "먼저 이전 장에서 했던 것처럼 와인 데이터셋을 판다스로 불러오고 훈련 세트와 테스트 세트로 나누자.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "pZGJZsI9cTbJ",
        "outputId": "e19b3c09-6feb-4e35-bb3f-6791dfafaa5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n랜덤 포레스트는 앙상블 학습의 대표 주자 중 하나로 안정적인 성능 덕에 널리 사용되고 있다.\\n이름 자체로 유추 가능하듯이, 랜덤 포레스트는 결정 트리를 랜덤하게 만들어 결정 트리(나무)의 \"숲\"을 만든다.\\n그리고 각 결정 트리의 예측을 사용해 최종 예측을 만든다.\\n그럼 랜덤 포레스트가 어떻게 숲을 구성하는지 알아보자.\\n\\n먼저 랜덤 포레스트는 각 트리를 훈련하기 위한 데이터를 랜덤하게 만드는데,\\n이 데이터를 만드는 방법이 독특하다.\\n우리가 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다.\\n이때 한 샘플이 중복되어 추출될 수도 있다.\\n\\n예를 들어, 1000개의 샘플이 들어있는 가방에서 100개의 샘플을 뽑는다면\\n먼저 1개를 뽑고, 뽑았던 1개를 다시 가방에 넣는다.\\n이런 식으로 계속해서 100개를 가방에서 뽑으면 중복된 샘플을 뽑을 수 있다.\\n이렇게 만들어진 샘플을 \"부트스트랩 샘플(bootstrap sample)\"이라고 부른다.\\n기본적으로 부트스트랩 샘플은 훈련 세트의 크기와 같게 만든다.\\n1000개의 샘플이 들어있는 가방에서 중복하여 1000개의 샘플을 뽑는다.\\n\\n(부트스트랩이란?\\n데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식을 의미한다.)\\n\\n또한 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음\\n이중에서 최선의 분할을 찾는다.\\n분류 모델인 RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 선택한다.\\n즉 4개의 특성이 있다면 노드마다 2개를 랜덤하게 선택하여 사용한다.\\n다만 회귀 모델인 RandomForestRegressor는 전체 특성을 사용한다.\\n\\n사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 이런 방식으로 훈련한다.\\n그다음 분류일 때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼는다.\\n회귀일 때는 단순히 각 트리의 예측을 평균한다.\\n\\n랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과댖거합되는 것을 막아주고,\\n검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다.\\n종종 기본 매개변수 설정만으로도 아주 좋은 결과를 낸다.\\n\\n그럼 이제 사이킷런의 RandomForestCalssifier 클래스를 화이트 와인을 분류하는 문제에 적용해 보자.\\n먼저 이전 장에서 했던 것처럼 와인 데이터셋을 판다스로 불러오고 훈련 세트와 테스트 세트로 나누자.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 업로드 및 훈련 세트와 테스트 세트 분할\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(\n",
        "    data, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "YgIHM1F1e1KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 교차 검증 수행\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "# return_train_score 매개변수를 True로 지정하면, 훈련 세트에 대한 점수도 같이 반환하여 과대적합을 파악하는 데 용이함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "158hDPVufX0I",
        "outputId": "f824e83b-10ed-4d8c-d890-4d7dd9047a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "랜덤 포레스트는 결정 트리의 앙상블이기 때문에 DecisionTreeClassifier가 제공하는 중요한 매개변수를 모두 제공한다.\n",
        "criterion, max_depth, max_features, min_samples_split, min_impurity_decrease, min_samples_leaf 등이다.\n",
        "또한 결정 트리의 큰 장점 중 하나인 특성 중요도를 계산해준다.\n",
        "랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것이다.\n",
        "앞의 랜덤 포레스트 모델을 훈련 세트에 훈련한 후 특성 중요도를 출력해 보자.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "dO7IAG5xftLs",
        "outputId": "75a13535-e0b1-49b9-ef9b-0cb71beaefeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n랜덤 포레스트는 결정 트리의 앙상블이기 때문에 DecisionTreeClassifier가 제공하는 중요한 매개변수를 모두 제공한다.\\ncriterion, max_depth, max_features, min_samples_split, min_impurity_decrease, min_samples_leaf 등이다.\\n또한 결정 트리의 큰 장점 중 하나인 특성 중요도를 계산해준다.\\n랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것이다.\\n앞의 랜덤 포레스트 모델을 훈련 세트에 훈련한 후 특성 중요도를 출력해 보자.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤 포레스트 모델을 훈련 세트에 훈련 후 특성 중요도 출력\n",
        "rf.fit(train_input, train_target)\n",
        "\n",
        "print(rf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnIStvyXg7g7",
        "outputId": "ff74f5be-a119-4db9-dbd5-5246fb14fd19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "5-1장에서 만든 특성 중요도와 비교해보면,\n",
        "두 번째 특성인 당도(sugar)의 중요도가 감소하고 알코올 도수와 pH 특성의 중요도가 상승했다.\n",
        "이러한 이유는 랜덤 포레스트가 특성의 일부를 랜덤하게 선택하여 결정 트리를 훈련하기 때문이다.\n",
        "그 결과 하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 얻는다.\n",
        "이는 과대적합을 줄이고 일반화 성능을 높이는 데 유용하다.\n",
        "\n",
        "RandomForestCalssifier에는 재미있는 기능이 하나 더 있는데, 자체적으로 모델을 평가하는 점수를 얻을 수 있다.\n",
        "랜덤 포레스트는 부트스트랩 샘플을 만들어 결정 트리를 훈련한다고 했다.\n",
        "이때 부트스트랩 샘플에 포함되지 않고 남는 샘플이 있다.\n",
        "이런 샘플을 \"OOB(out of bag) 샘플\"이라고 한다.\n",
        "이 남는 샘플을 사용하여 부트스트랩 샘플로 훈련한 결정 트리를 평가할 수 있다.\n",
        "마치 검증 세트의 역할을 하는 셈이다!\n",
        "\n",
        "이 점수를 얻으려면 RandomForestClassifier 클래스의 oob_score 매개변수를 True로 지정해야 한다.\n",
        "이렇게 하면 랜덤 포레스트는 각 결정 트리의 OOB 점수를 평균하여 출력한다.\n",
        "oob_score=True로 지정하고 모델을 훈련하여 OOB 점수를 출력해 보자.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "izE2B01EhD2z",
        "outputId": "f4f74e14-47d7-48a2-d5a8-6c4d1f3f9b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n5-1장에서 만든 특성 중요도와 비교해보면,\\n두 번째 특성인 당도(sugar)의 중요도가 감소하고 알코올 도수와 pH 특성의 중요도가 상승했다.\\n이러한 이유는 랜덤 포레스트가 특성의 일부를 랜덤하게 선택하여 결정 트리를 훈련하기 때문이다.\\n그 결과 하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 얻는다.\\n이는 과대적합을 줄이고 일반화 성능을 높이는 데 유용하다.\\n\\nRandomForestCalssifier에는 재미있는 기능이 하나 더 있는데, 자체적으로 모델을 평가하는 점수를 얻을 수 있다.\\n랜덤 포레스트는 부트스트랩 샘플을 만들어 결정 트리를 훈련한다고 했다.\\n이때 부트스트랩 샘플에 포함되지 않고 남는 샘플이 있다.\\n이런 샘플을 \"OOB(out of bag) 샘플\"이라고 한다.\\n이 남는 샘플을 사용하여 부트스트랩 샘플로 훈련한 결정 트리를 평가할 수 있다.\\n마치 검증 세트의 역할을 하는 셈이다!\\n\\n이 점수를 얻으려면 RandomForestClassifier 클래스의 oob_score 매개변수를 True로 지정해야 한다.\\n이렇게 하면 랜덤 포레스트는 각 결정 트리의 OOB 점수를 평균하여 출력한다.\\noob_score=True로 지정하고 모델을 훈련하여 OOB 점수를 출력해 보자.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OOB 점수 출력\n",
        "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
        "rf.fit(train_input, train_target)\n",
        "\n",
        "print(rf.oob_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrSUmgAdiMjT",
        "outputId": "9b2c7157-e5e5-4e2c-ae84-d81e96c221b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8934000384837406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "교차 검증에서 얻은 점수와 매우 비슷한 결과를 얻었다.\n",
        "OOB 점수를 사용하면 교차 검증을 대신할 수 있어서\n",
        "결과적으로 훈련 세트에 더 많은 샘플을 사용할 수 있다.\n",
        "\n",
        "다음에 알아볼 앙상블 학습은 랜덤 포레스트와 아주 비슷한 엑스트라 트리이다.\n",
        "\"엑스트라 트리(Extra Trees)\"는 랜덤 포레스트와 매우 비슷하게 동작한다.\n",
        "기본적으로 100개의 결정 트리를 훈련한다.\n",
        "랜덤 포레스트와 동일하게 결정 트리가 제공하는 대부분의 매개변수를 지원한다.\n",
        "또한 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는 데 사용한다.\n",
        "\n",
        "차이점은 부트스트랩 샘플을 사용하지 않는다는 점이다.\n",
        "즉 각 결정 트리를 맏늘 때 전체 훈련 세트를 사용한다.\n",
        "대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할한다!\n",
        "5-2장의 확인 문제에서 DecisionTreeClassifier의 splitter 매개변수를 'random'으로 지정한 바 있다.\n",
        "엑스트라 트리가 사용하는 결정 트리가 바로 splitter='random'인 결정 트리이다.\n",
        "하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지겠지만\n",
        "많은 트리를 앙상블 하기 때문에 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다.\n",
        "사이킷런에서 제공하는 엑스트라 트리는 ExtraTreesClassifier이다.\n",
        "이 모델의 교차 검증 점수를 확인해 보자.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "_WmfGpBoibcM",
        "outputId": "42c57665-a61f-404d-d2f9-ec2c21077f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n교차 검증에서 얻은 점수와 매우 비슷한 결과를 얻었다.\\nOOB 점수를 사용하면 교차 검증을 대신할 수 있어서\\n결과적으로 훈련 세트에 더 많은 샘플을 사용할 수 있다.\\n\\n다음에 알아볼 앙상블 학습은 랜덤 포레스트와 아주 비슷한 엑스트라 트리이다.\\n\"엑스트라 트리(Extra Trees)\"는 랜덤 포레스트와 매우 비슷하게 동작한다.\\n기본적으로 100개의 결정 트리를 훈련한다.\\n랜덤 포레스트와 동일하게 결정 트리가 제공하는 대부분의 매개변수를 지원한다.\\n또한 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는 데 사용한다.\\n\\n차이점은 부트스트랩 샘플을 사용하지 않는다는 점이다.\\n즉 각 결정 트리를 맏늘 때 전체 훈련 세트를 사용한다.\\n대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할한다!\\n5-2장의 확인 문제에서 DecisionTreeClassifier의 splitter 매개변수를 \\'random\\'으로 지정한 바 있다.\\n엑스트라 트리가 사용하는 결정 트리가 바로 splitter=\\'random\\'인 결정 트리이다.\\n하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지겠지만\\n많은 트리를 앙상블 하기 때문에 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다.\\n사이킷런에서 제공하는 엑스트라 트리는 ExtraTreesClassifier이다.\\n이 모델의 교차 검증 점수를 확인해 보자.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 엑스트라 트리 모델의 교차 검증 점수 확인\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(et, train_input, train_target,\n",
        "                        return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNp94nhbjawW",
        "outputId": "8cc38218-cb2c-437d-a8eb-6a8319e96210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "랜덤 포레스트와 비슷한 결과를 얻었다.\n",
        "이 예제는 특성이 많지 않아 두 모델의 차이가 크지 않다.\n",
        "보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정트리를 훈련해야 한다.\n",
        "하지만 랜덤하게 노드를 분할하기 때문에 빠른 계산 속도가 엑스트라 트리의 장점이다.\n",
        "\n",
        "(결정 트리는 최적의 분할을 찾는 데 시간을 많이 소모한다.\n",
        "특히 고려해야 할 특성의 개수가 많을 때 더 그렇다.)\n",
        "\n",
        "엑스트라 트리도 랜덤 포레스트와 마찬가지로 특성 중요도를 제공한다.\n",
        "순서는 [알코올 도수, 당도, pH]인데, 결과를 보면 엑스트라 트리도 결정 트리보다 당도에 대한 의존성이 작다.\n",
        "\n",
        "엑스트라 트리의 회귀 버전은 ExtraTreesRegressor 클래스이다.\n",
        "\n",
        "지금까지 2개의 앙상블 학습을 알아보았다.\n",
        "이제 이 둘과 다른 방식을 사용하는 앙상블 학습을 알아보겠다.\n",
        "먼저 그레이디언트 부스팅이다.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "4n6p0CF0j5xy",
        "outputId": "a2b4a6b5-ff43-49a6-ff28-623952d1c461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n랜덤 포레스트와 비슷한 결과를 얻었다.\\n이 예제는 특성이 많지 않아 두 모델의 차이가 크지 않다.\\n보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정트리를 훈련해야 한다.\\n하지만 랜덤하게 노드를 분할하기 때문에 빠른 계산 속도가 엑스트라 트리의 장점이다.\\n\\n(결정 트리는 최적의 분할을 찾는 데 시간을 많이 소모한다.\\n특히 고려해야 할 특성의 개수가 많을 때 더 그렇다.)\\n\\n엑스트라 트리도 랜덤 포레스트와 마찬가지로 특성 중요도를 제공한다.\\n순서는 [알코올 도수, 당도, pH]인데, 결과를 보면 엑스트라 트리도 결정 트리보다 당도에 대한 의존성이 작다.\\n\\n엑스트라 트리의 회귀 버전은 ExtraTreesRegressor 클래스이다.\\n\\n지금까지 2개의 앙상블 학습을 알아보았다.\\n이제 이 둘과 다른 방식을 사용하는 앙상블 학습을 알아보겠다.\\n먼저 그레이디언트 부스팅이다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성 중요도 출력\n",
        "et.fit(train_input, train_target)\n",
        "\n",
        "print(et.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TEpqK4lknQf",
        "outputId": "7a70d3e4-dd64-420c-8a16-5fde33b8069c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\"그레이디언트 부스팅(gradient boosting)\"은 깊이가 얕은 결정 트리를 사용하여\n",
        "이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다.\n",
        "사이킷런의 GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리를 100개 사용한다.\n",
        "깊이가 얕은 결정 트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다.\n",
        "\n",
        "그레이디언트란 이름답게 4장에서 배웠던 \"경사 하강법\"을 사용하여 트리를 앙상블에 추가한다.\n",
        "분류에서는 로지스틱 손실 함수를 사용하고, 회귀에서는 평균 제곱 오차 함수를 사용한다.\n",
        "\n",
        "4장에서 경사 하강법은 손실 함수를 산으로 정의하고 가장 낮은 곳을 찾아 내려오는 과정으로 설명했다.\n",
        "이때 가장 낮은 곳을 찾아 내려오는 방법은 모델의 가중치와 절편을 조금씩 바꾸는 것이다.\n",
        "그레이디언트 부스팅은 결정 트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동한다.\n",
        "손실 함수의 낮은 곳으로 천천히 조금씩 이동해야 하는 것처럼 그레이디언트 부스팅도 마찬가지이다.\n",
        "그래서 깊이가 얕은 트리를 사용하는 것이다!\n",
        "또한 학습률 매개변수(learning rate)로 속도를 조절한다.\n",
        "\n",
        "사이킷런에서 제공하는 GradientBoostingClassifier을 사용해 와인 데이터셋의 교차 검증 점수를 확인해 보자.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "Em1gtUvlktVD",
        "outputId": "212b69c0-72af-41de-d86c-c575f8a026b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\"그레이디언트 부스팅(gradient boosting)\"은 깊이가 얕은 결정 트리를 사용하여\\n이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다.\\n사이킷런의 GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리를 100개 사용한다.\\n깊이가 얕은 결정 트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다.\\n\\n그레이디언트란 이름답게 4장에서 배웠던 \"경사 하강법\"을 사용하여 트리를 앙상블에 추가한다.\\n분류에서는 로지스틱 손실 함수를 사용하고, 회귀에서는 평균 제곱 오차 함수를 사용한다.\\n\\n4장에서 경사 하강법은 손실 함수를 산으로 정의하고 가장 낮은 곳을 찾아 내려오는 과정으로 설명했다.\\n이때 가장 낮은 곳을 찾아 내려오는 방법은 모델의 가중치와 절편을 조금씩 바꾸는 것이다.\\n그레이디언트 부스팅은 결정 트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동한다.\\n손실 함수의 낮은 곳으로 천천히 조금씩 이동해야 하는 것처럼 그레이디언트 부스팅도 마찬가지이다.\\n그래서 깊이가 얕은 트리를 사용하는 것이다!\\n또한 학습률 매개변수(learning rate)로 속도를 조절한다.\\n\\n사이킷런에서 제공하는 GradientBoostingClassifier을 사용해 와인 데이터셋의 교차 검증 점수를 확인해 보자.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 와인 데이터셋 교차 검증 점수 확인\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target,\n",
        "                        return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ8Kyw8ol5DL",
        "outputId": "5ce4a550-e2cf-437d-d040-3f1466a2a3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "교차 검증 점수를 확인한 결과, 거의 과대적합이 되지 않는 것을 확인할 수 있따.\n",
        "그레이디언트 부스팅은 결정 트리의 개수를 늘려도 과대적합에 매우 강하다.\n",
        "학습률을 증가시키고 트리의 개수를 늘리면 조금 더 성능이 향상될 수 있다.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ZthAoof8mRAp",
        "outputId": "35308381-d822-4033-964b-e7e2d70292f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n교차 검증 점수를 확인한 결과, 거의 과대적합이 되지 않는 것을 확인할 수 있따.\\n그레이디언트 부스팅은 결정 트리의 개수를 늘려도 과대적합에 매우 강하다.\\n학습률을 증가시키고 트리의 개수를 늘리면 조금 더 성능이 향상될 수 있다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습률 증가 및 트리 개수 증가\n",
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "\n",
        "scores = cross_validate(gb, train_input, train_target,\n",
        "                        return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLj6pCmzn2zK",
        "outputId": "6cfd8c77-1cae-4e28-8c4a-c24b6b69f203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "결정 트리 개수를 500개로 5개나 늘렸지만 과대적합을 잘 억제하고 있다.\n",
        "학습률 learning_rate의 기본값은 0.1이다.\n",
        "그레이디언트 부스팅도 특성 중요도를 제공한다.\n",
        "결과에서 볼 수 있듯이 그레이디언트 부스팅이 랜덤 포레스트보다 일부 특성(당도)에 더 집중한다.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "SJi_WnTGoQhQ",
        "outputId": "80939a6f-fcc1-48db-abf0-16a98f5ca2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n결정 트리 개수를 500개로 5개나 늘렸지만 과대적합을 잘 억제하고 있다.\\n학습률 learning_rate의 기본값은 0.1이다.\\n그레이디언트 부스팅도 특성 중요도를 제공한다.\\n결과에서 볼 수 있듯이 그레이디언트 부스팅이 랜덤 포레스트보다 일부 특성(당도)에 더 집중한다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성 중요도 확인\n",
        "gb.fit(train_input, train_target)\n",
        "\n",
        "print(gb.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EYugLTAosAw",
        "outputId": "b68a3db6-1085-4d5d-d259-2191aead599c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.15887763 0.6799705  0.16115187]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "흥미로운 매개변수가 하나 있다.\n",
        "트리 훈련에 사용할 훈련 세트의 비율을 정하는 subsample이다.\n",
        "이 매개변수의 기본값은 1.0으로 전체 훈련 세트를 사용한다.\n",
        "하지만 subsample이 1보다 작으면 훈련 세트의 일부를 사용한다.\n",
        "이는 마치 경사 하강법 단계마다 일부 샘플을 랜덤하게 선택하여 진행하는\n",
        "확률적 경사 하강법이나 미니배치 경사 하강법과 비슷하다.\n",
        "\n",
        "일반적으로 그레이디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있다.\n",
        "하지만 순서대로 트리를 추가하기 때문에 훈련 속도가 느리다는 단점이 있다.\n",
        "즉 GradientBoostingClassifier에는 n_jobs 매개변수가 없다.\n",
        "그레이디언트 부스팅의 회귀 버전은 GradientBoostingRegressor이다.\n",
        "그레이디언트 부스팅의 속도와 성능을 더욱 개선한 것이 다음에 살펴볼 히스토그램 기반 그레이디언트 부스팅이다.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "c5X28GzCowdY",
        "outputId": "3fef8d65-d98d-46e8-ab67-38f454808d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n흥미로운 매개변수가 하나 있다.\\n트리 훈련에 사용할 훈련 세트의 비율을 정하는 subsample이다.\\n이 매개변수의 기본값은 1.0으로 전체 훈련 세트를 사용한다.\\n하지만 subsample이 1보다 작으면 훈련 세트의 일부를 사용한다.\\n이는 마치 경사 하강법 단계마다 일부 샘플을 랜덤하게 선택하여 진행하는\\n확률적 경사 하강법이나 미니배치 경사 하강법과 비슷하다.\\n\\n일반적으로 그레이디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있다.\\n하지만 순서대로 트리를 추가하기 때문에 훈련 속도가 느리다는 단점이 있다.\\n즉 GradientBoostingClassifier에는 n_jobs 매개변수가 없다.\\n그레이디언트 부스팅의 회귀 버전은 GradientBoostingRegressor이다.\\n그레이디언트 부스팅의 속도와 성능을 더욱 개선한 것이 다음에 살펴볼 히스토그램 기반 그레이디언트 부스팅이다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\"히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)\"은\n",
        "정형 데이터를 다루는 머신러닝 알고리즘 중 가장 인기가 높은 알고리즘이다.\n",
        "히스토그램 기반 그레이디언트 부스팅은 먼저 특성을 256개의 구간으로 나눈다.\n",
        "따라서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다.\n",
        "히스토그램 기반 그레이디언트 부스팅은 256개의 구간 중 하나를 떼어 놓고 누락된 값을 위해 사용한다.\n",
        "따라서 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없다!\n",
        "\n",
        "사이킷런의 히스토그램 기반 그레이디언트 부스팅 클래스는 HistGradientBoostingClassifier이다.\n",
        "일반적으로 이 클래스는 기본 매개변수에서 안정적인 성능을 얻을 수 있다.\n",
        "이 클래스에는 트리의 개수를 지정하는데, n_estimators 대신에 부스팅 반복 횟수를 지정하는 max_iter를 사용한다.\n",
        "성능을 높이려면 max_iter 매개변수를 테스트해 보라.\n",
        "\n",
        "그럼 와인 데이터셋에 HistGradientBoostingClassifier 클래스를 적용해 보자.\n",
        "사이킷런의 히스토그램 기반 그레이디언트 부스팅은 아직 테스트 과정에 있다.\n",
        "이 클래스를 사용하려면 sklearn.experimental 패키지 아래에 있는 enable_hist_gradient_boosting 모듈을 임포트 해야 한다.\n",
        "\n",
        "(사이킷런 1.0에서 히스토그램 기반 부스팅이 테스트 과정에서 벗어남)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "ET_vO7yHpYwS",
        "outputId": "6c68e947-3455-4e30-afb3-9e63364ad796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\"히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)\"은\\n정형 데이터를 다루는 머신러닝 알고리즘 중 가장 인기가 높은 알고리즘이다.\\n히스토그램 기반 그레이디언트 부스팅은 먼저 특성을 256개의 구간으로 나눈다.\\n따라서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다.\\n히스토그램 기반 그레이디언트 부스팅은 256개의 구간 중 하나를 떼어 놓고 누락된 값을 위해 사용한다.\\n따라서 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없다!\\n\\n사이킷런의 히스토그램 기반 그레이디언트 부스팅 클래스는 HistGradientBoostingClassifier이다.\\n일반적으로 이 클래스는 기본 매개변수에서 안정적인 성능을 얻을 수 있다.\\n이 클래스에는 트리의 개수를 지정하는데, n_estimators 대신에 부스팅 반복 횟수를 지정하는 max_iter를 사용한다.\\n성능을 높이려면 max_iter 매개변수를 테스트해 보라.\\n\\n그럼 와인 데이터셋에 HistGradientBoostingClassifier 클래스를 적용해 보자.\\n사이킷런의 히스토그램 기반 그레이디언트 부스팅은 아직 테스트 과정에 있다.\\n이 클래스를 사용하려면 sklearn.experimental 패키지 아래에 있는 enable_hist_gradient_boosting 모듈을 임포트 해야 한다.\\n\\n(사이킷런 1.0에서 히스토그램 기반 부스팅이 테스트 과정에서 벗어남)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylNUPziZqb3-",
        "outputId": "350d05a9-62c4-41e3-967c-0380c16e2999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "과대적합을 잘 억제하면서 그레이디언트 부스팅보다 조금 더 높은 성능을 제공하는 것을 확인할 수 있다.\n",
        "특성 중요도를 확인해 보자.\n",
        "\n",
        "히스토그램 기반 그레이디언트 부스팅의 특성 중요도를 계산하기 위해\n",
        "permutation_importance() 함수를 사용할 것이다.\n",
        "이 함수는 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰하여\n",
        "어떤 특성이 중요한지를 계산한다.\n",
        "훈련 세트뿐만 아니라 테스트 세트에도 적용할 수 있고 사이킷런에서 제공하는 추정기 모델에 모두 사용할 수 있다.\n",
        "\n",
        "먼저 히스토그램 기반 그레이디언트 부스팅 모델을 훈련 후 훈련 세트에서 특성 중요도를 계산해 보겠다.\n",
        "n_repeats 매개변수는 랜덤하게 섞을 횟수를 지정한다.\n",
        "여기서는 10으로 지정하겠다.\n",
        "기본값은 5이다.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "FJdUZVivqjSA",
        "outputId": "e6e41ace-c77e-47d9-fc8b-b8ba2b0bb81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n과대적합을 잘 억제하면서 그레이디언트 부스팅보다 조금 더 높은 성능을 제공하는 것을 확인할 수 있다.\\n특성 중요도를 확인해 보자.\\n\\n히스토그램 기반 그레이디언트 부스팅의 특성 중요도를 계산하기 위해 \\npermutation_importance() 함수를 사용할 것이다.\\n이 함수는 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰하여\\n어떤 특성이 중요한지를 계산한다.\\n훈련 세트뿐만 아니라 테스트 세트에도 적용할 수 있고 사이킷런에서 제공하는 추정기 모델에 모두 사용할 수 있다.\\n\\n먼저 히스토그램 기반 그레이디언트 부스팅 모델을 훈련 후 훈련 세트에서 특성 중요도를 계산해 보겠다.\\nn_repeats 매개변수는 랜덤하게 섞을 횟수를 지정한다.\\n여기서는 10으로 지정하겠다.\\n기본값은 5이다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 모델 훈련 후 훈련 세트에서 특성 중요도 확인\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "hgb.fit(train_input, train_target)\n",
        "\n",
        "result = permutation_importance(hgb, train_input, train_target,\n",
        "                                n_repeats=10, random_state=42, n_jobs=-1)\n",
        "\n",
        "print(result.importances_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pj3FHRorGj-",
        "outputId": "3792d06a-58e2-4929-a9bf-2c8232fc49a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "permutation_importance() 함수가 반환하는 객체는 반복하여 얻은\n",
        "특성 중요도(importances), 평균(importances_mean), 표준 편차(importances_std)를 담고 있다.\n",
        "평균을 출력해 보면 랜덤 포레스트와 비슷한 비율임을 알 수 있다.\n",
        "이번에는 테스트 세트에서 특성 중요도를 계산해 보겠다.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dNBjfQy4rbZf",
        "outputId": "1183e74d-c959-4b8c-cf89-e1ce589f8c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npermutation_importance() 함수가 반환하는 객체는 반복하여 얻은\\n특성 중요도(importances), 평균(importances_mean), 표준 편차(importances_std)를 담고 있다.\\n평균을 출력해 보면 랜덤 포레스트와 비슷한 비율임을 알 수 있다.\\n이번에는 테스트 세트에서 특성 중요도를 계산해 보겠다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 세트에서 특성 중요도 확인\n",
        "result = permutation_importance(hgb, test_input, test_target,\n",
        "                                n_repeats=10, random_state=42, n_jobs=-1)\n",
        "\n",
        "print(result.importances_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEXEPeUlsKrC",
        "outputId": "99de02df-a045-477c-94ea-0ffd1bf8c799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "테스트 세트의 결과를 보면 그레이디언트 부스팅과 비슷하게 조금 더 당도에 집중하고 있다는 것을 알 수 있다.\n",
        "이런 분석을 통해 모델을 실전에 투입했을 때 어떤 특성에 관심을 둘지 예상할 수 있다.\n",
        "\n",
        "그럼 HistGradientBoostingClassifier을 사용해 테스트 세트에서의 성능을 최종 확인해 보자.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "tMMdpOWNsYif",
        "outputId": "cdaa8aa2-4a8c-4b94-9b45-a618af59fbc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n테스트 세트의 결과를 보면 그레이디언트 부스팅과 비슷하게 조금 더 당도에 집중하고 있다는 것을 알 수 있다.\\n이런 분석을 통해 모델을 실전에 투입했을 때 어떤 특성에 관심을 둘지 예상할 수 있다.\\n\\n그럼 HistGradientBoostingClassifier을 사용해 테스트 세트에서의 성능을 최종 확인해 보자.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hgb.score(test_input, test_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdVv5JuCspJF",
        "outputId": "1a3f393a-0d1f-4ebe-8b25-e49b0de646c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "테스트 세트에서는 약 87% 정확도를 얻었다.\n",
        "실전에 투입하면 성능은 이보다는 조금 더 낮을 것이다.\n",
        "앙상블 모델은 확실히 단일 결정 트리보다 좋은 결과를 얻을 수 있다!\n",
        "(5-2 장 랜덤 서치에서의 테스트 정확도는 86%였음)\n",
        "\n",
        "히스토 그램 기반 그레이디언트 부스팅의 회귀 버전은 HistGradientBoostingRegressor 클래스이다.\n",
        "사이킷런 말고도 그레이디언트 부스팅 알고리즘을 구현한 라이브러리가 여럿 존재한다.\n",
        "\n",
        "가장 대표적인 것은 XGBoost이다.\n",
        "이 라이브러리도 코랩에서 사용 가능하며 사이킷런의 cross_validate() 함수와 함께 사용 가능하다.\n",
        "XGBoost는 다양한 부스팅 알고리즘을 지원한다.\n",
        "tree_method 매개변수를 'hist'로 지정하면 히스토그램 기반 그레이디언트 부스팅을 사용할 수 있다.\n",
        "그럼 XGBoost를 사용해 와인 데이터의 교차 검증 점수를 확인해 보자.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "WdB4nlYNssek",
        "outputId": "bf1b0926-f881-465a-b93a-afd5c466300a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n테스트 세트에서는 약 87% 정확도를 얻었다.\\n실전에 투입하면 성능은 이보다는 조금 더 낮을 것이다.\\n앙상블 모델은 확실히 단일 결정 트리보다 좋은 결과를 얻을 수 있다!\\n(5-2 장 랜덤 서치에서의 테스트 정확도는 86%였음)\\n\\n히스토 그램 기반 그레이디언트 부스팅의 회귀 버전은 HistGradientBoostingRegressor 클래스이다.\\n사이킷런 말고도 그레이디언트 부스팅 알고리즘을 구현한 라이브러리가 여럿 존재한다.\\n\\n가장 대표적인 것은 XGBoost이다. \\n이 라이브러리도 코랩에서 사용 가능하며 사이킷런의 cross_validate() 함수와 함께 사용 가능하다.\\nXGBoost는 다양한 부스팅 알고리즘을 지원한다.\\ntree_method 매개변수를 'hist'로 지정하면 히스토그램 기반 그레이디언트 부스팅을 사용할 수 있다.\\n그럼 XGBoost를 사용해 와인 데이터의 교차 검증 점수를 확인해 보자.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.5.2  # Or an earlier compatible version\n",
        "!pip install --upgrade skorch imbalanced-learn  # Example\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
        "scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "Sxv5gjGJtVZa",
        "outputId": "a6cd51d3-e62e-4a9f-da82-77709a780a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (3.5.0)\n",
            "Collecting skorch\n",
            "  Downloading skorch-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.13.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.67.1)\n",
            "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
            "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Downloading skorch-1.0.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: skorch, sklearn-compat, imbalanced-learn\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.12.4\n",
            "    Uninstalling imbalanced-learn-0.12.4:\n",
            "      Successfully uninstalled imbalanced-learn-0.12.4\n",
            "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3 skorch-1.0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'super' object has no attribute '__sklearn_tags__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-264dd68a8ef1>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0;31m# the function to delegate validation to the estimator, but we replace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0;31m# the name of the estimator by the name of the function in the error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# message to avoid confusion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3635.5\u001b[0m\u001b[0;34m...\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3573.3\u001b[0m\u001b[0;34m...\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m6114.7\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_r2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0;36m0.28009951\u001b[0m \u001b[0;36m0.3908844\u001b[0m  \u001b[0;36m0.22784907\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \"\"\"\n\u001b[1;32m    349\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_params_groups_deprecation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mis_classifier\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mOutlierMixin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \"\"\"Mixin class for all outlier detection estimators in scikit-learn.\n\u001b[0;32m-> 1237\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mmixin\u001b[0m \u001b[0mdefines\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mfunctionality\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py\u001b[0m in \u001b[0;36mget_tags\u001b[0;34m(estimator)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m__sklearn_tags__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"no_validation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"no_validation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "널리 사용하는 또 다른 히스토그램 기반 그레이디언트 부스팅 라이브러리는 마이크로소프트에서 만든 LightGBM이다.\n",
        "LightGBM은 빠르고 최신 기술을 많이 적용하고 있어 인기가 매우 높다.\n",
        "LightGBM도 코랩에 이미 설치되어 있다.\n",
        "\n",
        "사이킷런의 히스토그램 기반 그레이디언트 부스팅이 LightGBM에서 영향을 많이 받았다.\n",
        "이제 히스토그램 기반 그레이디언트 부스팅까지 4개의 앙상블을 모두 다루어 보았다!\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "g4DPYW0ItpU-",
        "outputId": "ba86889f-85d5-4dd9-f933-84c4f82de9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n널리 사용하는 또 다른 히스토그램 기반 그레이디언트 부스팅 라이브러리는 마이크로소프트에서 만든 LightGBM이다.\\nLightGBM은 빠르고 최신 기술을 많이 적용하고 있어 인기가 매우 높다.\\nLightGBM도 코랩에 이미 설치되어 있다.\\n\\n사이킷런의 히스토그램 기반 그레이디언트 부스팅이 LightGBM에서 영향을 많이 받았다.\\n이제 히스토그램 기반 그레이디언트 부스팅까지 4개의 앙상블을 모두 다루어 보았다!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hgp7zt_umMw",
        "outputId": "753fb343-0628-4f0b-ecc6-ed950657b74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The LGBMClassifier or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.935828414851749 0.8801251203079884\n"
          ]
        }
      ]
    }
  ]
}