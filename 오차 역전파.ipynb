{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/U0zFPq/RF5SO3Vw6Z3Q2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoahLee99/ML-DL-studylog/blob/main/%EC%98%A4%EC%B0%A8%20%EC%97%AD%EC%A0%84%ED%8C%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **오차 역전파**"
      ],
      "metadata": {
        "id": "7wFNuH-iOvFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 한 번의 \"순전파(forward propagtion)\"가 일어나면 각 층의 가중치 초깃값이 정해진다.\n",
        "<br> 이 초깃값의 가중치로 만들어진 값(출력 값)과 실제 값을 비교해 출력층의 오차를 계산한다.\n",
        "\n",
        "* 이때 계산된 **출력층의 오차를 최소화시키기 위해**\n",
        "<br> 출력층에서 입력층으로 거꾸로 전파하여 <br> 각 층의 가중치(weight)와 편향(bias)을 수정(업데이트)하는\n",
        "<br> 알고리즘이 바로 **\"역전파(backward propagation)\"**이다.\n",
        "\n",
        "* 역전파는 **\"연쇄 법칙(chain rule)\"**을 기반으로 하며,\n",
        "<br> \"손실 함수(loss function)\"의 기울기를 계산하여\n",
        "<br> 가중치가 손실 함수의 변화에 따라 어떻게 변화해야 하는지를 결정한다.\n",
        "\n",
        "* 역전파에 한계점이 존재하는데,\n",
        "<br> 출력층에서 시작된 가중치 업데이트가 처음 층까지 전달되지 않는 현상이 생기는 문제가 있다.\n",
        "<br> 이는 활성화 함수로 사용된 시그모이드 함수의 특성 때문이다.\n",
        "<br> 시그모이드(0~1 사이의 값만 존재) 함수를 미분하면 최대치는 0.25이다.\n",
        "<br> 시그모이드 함수의 값들은 1보다 작으므로 계속 곱하다 보면 0에 가까워진다.\n",
        "<br> 따라서 여러 층을 거칠수록 기울기가 사라지는 **\"기울기 소실(vanishing gradient)\"** 문제가 발생한다.\n",
        "<br> 이를 해결하고자 **\"렐루(ReLU)\"**라는 활성화 함수가 탄생했다.\n",
        "\n",
        "* 렐루는 x가 0보다 작을 때 모든 값을 0으로 처리하고,\n",
        "<br> 0보다 큰 값은 x를 그대로 사용하는 방법이다.\n",
        "<br> 따라서 x가 0보다 크기만 하면 미분 값은 1이 되어 기울기 소실 문제를 해결할 수 있다."
      ],
      "metadata": {
        "id": "Ps-EXm7POx2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR 문제를 사용해 오차 역전파를 파이썬 코드로 확인하고자 한다."
      ],
      "metadata": {
        "id": "_5aSCz4qSi5R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 환경 변수 설정하기"
      ],
      "metadata": {
        "id": "9L9ym1WhSzFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 값이 될 XOR 진리표\n",
        "# x1 = [0, 0, 1, 1]\n",
        "# x2 = [0, 1, 0, 1]\n",
        "# 결과값 = [0, 1, 1, 0]"
      ],
      "metadata": {
        "id": "gwrGNXRzS1qu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR 진리표를 두개의 입력 값과 한 개의 타깃 값으로 설정\n",
        "\n",
        "# 입력 값 및 타깃 값\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "data = [\n",
        "    [[0, 0], [0]],\n",
        "    [[0, 1], [1]],\n",
        "    [[1, 0], [1]],\n",
        "    [[1, 1], [0]]\n",
        "]\n",
        "\n",
        "# 실행 횟수(iterations), 학습률(lr), 모멘텀 계수(mo) 설정\n",
        "iterations=5000\n",
        "lr=0.1\n",
        "mo=0.4"
      ],
      "metadata": {
        "id": "fsg_s9PSTSW3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 활성화 함수 지정 - 1.시그모이드\n",
        "\n",
        "# 미분할 때와 아닐 때의 각각의 값\n",
        "def sigmoid(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        return x * (1 - x)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# 활성화 함수 지정 - 2.tanh\n",
        "# tanh 함수의 미분은 1 - (활성화 함수 출력의 제곱)\n",
        "def tanh(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        return 1 - x ** 2\n",
        "    return np.tanh(x)\n",
        "\n",
        "# 가중치 배열 만드는 함수\n",
        "def makeMatrix(i, j, fill=0.0):\n",
        "    mat = []\n",
        "    for i in range(i):\n",
        "        mat.append([fill] * j)\n",
        "    return mat"
      ],
      "metadata": {
        "id": "OwIPNc5FUCl3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 파이썬 코드로 실행하는 신경망"
      ],
      "metadata": {
        "id": "DTzPQoZ3UY1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망의 실행\n",
        "class NeuralNetwork:\n",
        "\n",
        "    # 초깃값의 지정\n",
        "    def __init__(self, num_x, num_yh, num_yo, bias=1):\n",
        "\n",
        "        # 입력값(num_x), 은닉층 초깃값(num_yh), 출력층 초깃값(num_yo), 바이어스\n",
        "        self.num_x = num_x + bias  # 바이어스는 1로 지정(본문 참조)\n",
        "        self.num_yh = num_yh\n",
        "        self.num_yo = num_yo\n",
        "\n",
        "        # 활성화 함수 초깃값\n",
        "        self.activation_input = [1.0] * self.num_x\n",
        "        self.activation_hidden = [1.0] * self.num_yh\n",
        "        self.activation_out = [1.0] * self.num_yo\n",
        "\n",
        "        # 가중치 입력 초깃값\n",
        "        self.weight_in = makeMatrix(self.num_x, self.num_yh)\n",
        "        for i in range(self.num_x):\n",
        "            for j in range(self.num_yh):\n",
        "                self.weight_in[i][j] = random.random()\n",
        "\n",
        "        # 가중치 출력 초깃값\n",
        "        self.weight_out = makeMatrix(self.num_yh, self.num_yo)\n",
        "        for j in range(self.num_yh):\n",
        "            for k in range(self.num_yo):\n",
        "                self.weight_out[j][k] = random.random()\n",
        "\n",
        "        # 모멘텀 SGD를 위한 이전 가중치 초깃값\n",
        "        self.gradient_in = makeMatrix(self.num_x, self.num_yh)\n",
        "        self.gradient_out = makeMatrix(self.num_yh, self.num_yo)\n",
        "\n",
        "    # 업데이트 함수\n",
        "    def update(self, inputs):\n",
        "\n",
        "        # 입력 레이어의 활성화 함수\n",
        "        for i in range(self.num_x - 1):\n",
        "            self.activation_input[i] = inputs[i]\n",
        "\n",
        "        # 은닉층의 활성화 함수\n",
        "        for j in range(self.num_yh):\n",
        "            sum = 0.0\n",
        "            for i in range(self.num_x):\n",
        "                sum = sum + self.activation_input[i] * self.weight_in[i][j]\n",
        "            # 시그모이드와 tanh 중에서 활성화 함수 선택\n",
        "            self.activation_hidden[j] = tanh(sum, False)\n",
        "\n",
        "        # 출력층의 활성화 함수\n",
        "        for k in range(self.num_yo):\n",
        "            sum = 0.0\n",
        "            for j in range(self.num_yh):\n",
        "                sum = sum + self.activation_hidden[j] * self.weight_out[j][k]\n",
        "            # 시그모이드와 tanh 중에서 활성화 함수 선택\n",
        "            self.activation_out[k] = tanh(sum, False)\n",
        "\n",
        "        return self.activation_out[:]\n",
        "\n",
        "    # 역전파의 실행\n",
        "    def backPropagate(self, targets):\n",
        "\n",
        "        # 델타 출력 계산\n",
        "        output_deltas = [0.0] * self.num_yo\n",
        "        for k in range(self.num_yo):\n",
        "            error = targets[k] - self.activation_out[k]\n",
        "            # 시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용\n",
        "            output_deltas[k] = tanh(self.activation_out[k], True) * error\n",
        "\n",
        "        # 은닉 노드의 오차 함수\n",
        "        hidden_deltas = [0.0] * self.num_yh\n",
        "        for j in range(self.num_yh):\n",
        "            error = 0.0\n",
        "            for k in range(self.num_yo):\n",
        "                error = error + output_deltas[k] * self.weight_out[j][k]\n",
        "                # 시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용\n",
        "            hidden_deltas[j] = tanh(self.activation_hidden[j], True) * error\n",
        "\n",
        "        # 출력 가중치 업데이트\n",
        "        for j in range(self.num_yh):\n",
        "            for k in range(self.num_yo):\n",
        "                gradient = output_deltas[k] * self.activation_hidden[j]\n",
        "                v = mo * self.gradient_out[j][k] - lr * gradient\n",
        "                self.weight_out[j][k] += v\n",
        "                self.gradient_out[j][k] = gradient\n",
        "\n",
        "        # 입력 가중치 업데이트\n",
        "        for i in range(self.num_x):\n",
        "            for j in range(self.num_yh):\n",
        "                gradient = hidden_deltas[j] * self.activation_input[i]\n",
        "                v = mo*self.gradient_in[i][j] - lr * gradient\n",
        "                self.weight_in[i][j] += v\n",
        "                self.gradient_in[i][j] = gradient\n",
        "\n",
        "        # 오차의 계산(최소 제곱법)\n",
        "        error = 0.0\n",
        "        for k in range(len(targets)):\n",
        "            error = error + 0.5 * (targets[k] - self.activation_out[k]) ** 2\n",
        "        return error\n",
        "\n",
        "    # 학습 실행\n",
        "    def train(self, patterns):\n",
        "        for i in range(iterations):\n",
        "            error = 0.0\n",
        "            for p in patterns:\n",
        "                inputs = p[0]\n",
        "                targets = p[1]\n",
        "                self.update(inputs)\n",
        "                error = error + self.backPropagate(targets)\n",
        "            if i % 500 == 0:\n",
        "                print('error: %-.5f' % error)\n",
        "    # 결괏값 출력\n",
        "    def result(self, patterns):\n",
        "        for p in patterns:\n",
        "            print('Input: %s, Predict: %s' % (p[0], self.update(p[0])))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # 두 개의 입력 값, 두 개의 레이어, 하나의 출력 값을 갖도록 설정\n",
        "    n = NeuralNetwork(2, 2, 1)\n",
        "\n",
        "    # 학습 실행\n",
        "    n.train(data)\n",
        "\n",
        "    # 결괏값 출력\n",
        "    n.result(data)\n",
        "\n",
        "\n",
        "# Reference: http://arctrix.com/nas/python/bpnn.py (Neil Schemenauer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT3v3ACkUg1g",
        "outputId": "1feac12a-68cd-41df-df6b-cfd92550059e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: 0.63006\n",
            "error: 0.04397\n",
            "error: 0.00240\n",
            "error: 0.00088\n",
            "error: 0.00051\n",
            "error: 0.00035\n",
            "error: 0.00027\n",
            "error: 0.00021\n",
            "error: 0.00018\n",
            "error: 0.00015\n",
            "Input: [0, 0], Predict: [0.0010564190342788123]\n",
            "Input: [0, 1], Predict: [0.9885356741580501]\n",
            "Input: [1, 0], Predict: [0.9885588149855168]\n",
            "Input: [1, 1], Predict: [0.002302951994419993]\n"
          ]
        }
      ]
    }
  ]
}